# Data Processing Pipeline Configuration
# Real-time data ingestion and analysis system

project "data-pipeline-system" {
    version = "2.1.0"
    author = "DataOps Team"
    description = "High-throughput data processing pipeline with ML integration"
}

agent "data-ingester" {
    model = "gpt-4"
    role = "Data Ingestion Specialist"
    temperature = 0.3
    max_tokens = 50000

    capabilities [
        "kafka-streaming"
        "data-validation"
        "schema-registry"
        "batch-processing"
        "real-time-ingestion"
    ]

    backstory {
        10 years of big data experience
        Processed petabytes of data
        Expert in Apache Kafka and streaming systems
        Built high-throughput data pipelines
    }

    tools = [
        "kafka"
        "apache-nifi"
        "debezium"
        "schema-registry"
        "data-quality-tools"
    ]
}

agent "data-transformer" {
    model = "claude-3-sonnet"
    role = "ETL Engineer"
    temperature = 0.5
    max_tokens = 75000

    capabilities [
        "sql-optimization"
        "data-cleansing"
        "feature-engineering"
        "data-normalization"
        "complex-joins"
    ]

    backstory {
        8 years in data engineering
        Expert in Apache Spark and distributed computing
        Optimized queries reducing processing time by 80%
        Led data warehouse migrations
    }

    tools = [
        "spark"
        "hive"
        "presto"
        "airflow"
        "dbt"
    ]
}

agent "ml-engineer" {
    model = "claude-3-opus"
    role = "Machine Learning Engineer"
    temperature = 0.6
    max_tokens = 100000

    capabilities [
        "feature-selection"
        "model-training"
        "hyperparameter-tuning"
        "model-validation"
        "prediction-pipelines"
    ]

    backstory {
        PhD in Machine Learning
        Published 20+ papers on ML systems
        Built ML pipelines processing billions of predictions daily
        Expert in production ML deployment
    }

    tools = [
        "python"
        "scikit-learn"
        "tensorflow"
        "mlflow"
        "kubernetes"
    ]
}

workflow "data-processing-pipeline" {
    trigger = "schedule:daily"

    step "data-ingestion" {
        agent = "data-ingester"
        task = "Ingest streaming data from multiple sources"
        timeout = 30m
        parallel = true
    }

    step "data-validation" {
        agent = "data-ingester"
        task = "Validate data quality and schema compliance"
        timeout = 15m
        depends_on = ["data-ingestion"]
    }

    step "data-transformation" {
        agent = "data-transformer"
        task = "Clean and transform data for analysis"
        timeout = 45m
        depends_on = ["data-validation"]

        retry {
            max_attempts = 3
            delay = 5m
            backoff = "exponential"
        }
    }

    step "feature-engineering" {
        agent = "ml-engineer"
        task = "Create features for ML models"
        timeout = 1h
        depends_on = ["data-transformation"]
    }

    step "model-inference" {
        agent = "ml-engineer"
        task = "Run ML models for predictions and insights"
        timeout = 30m
        depends_on = ["feature-engineering"]
        parallel = true
    }

    step "results-storage" {
        agent = "data-transformer"
        task = "Store processed results and insights"
        timeout = 20m
        depends_on = ["model-inference"]
    }

    pipeline {
        data-ingestion -> data-validation -> data-transformation -> feature-engineering -> model-inference -> results-storage
    }
}

crew "data-team" {
    agents [
        "data-ingester"
        "data-transformer"
        "ml-engineer"
    ]

    process = "parallel"
    max_iterations = 5
    verbose = true
}

memory {
    provider = "mongodb"
    connection = "mongodb://localhost:27017/data_pipeline"

    embeddings {
        model = "text-embedding-3-small"
        dimensions = 1536
        batch_size = 100
    }

    cache_size = 10000
    persistence = true
}

context "production" {
    environment = "prod"
    debug = false
    max_tokens = 200000

    secrets {
        kafka_credentials = $KAFKA_CREDENTIALS
        database_password = $DATABASE_PASSWORD
        mlflow_token = $MLFLOW_API_TOKEN
    }

    variables {
        kafka_brokers = "kafka-cluster.company.com:9092"
        mongodb_uri = "mongodb://prod-db.company.com:27017"
        batch_size = 1000
        processing_timeout = 2h
        retry_attempts = 5
    }
}